---
title: "Lab 9 - HPC"
author: "Yufei Liu"
output: 
  # pdf_document: default
  # html_document: default
  tufte::tufte_html:
    css: style.css
link-citations: yes
---

# Learning goals

In this lab, you are expected to learn/put in practice the following skills:

- Evaluate whether a problem can be parallelized or not.
- Practice with the parallel package.
- Use Rscript to submit jobs.

```{r eval=TRUE, echo=FALSE}
# install any missing packages
# install.packages("microbenchmark")
library(microbenchmark)
library(ggplot2)
library(parallel)
```

## Problem 1: Think

Give yourself a few minutes to think about what you learned about parallelization. List three
examples of problems that you believe may be solved using parallel computing,
and check for packages on the HPC CRAN task view that may be related to it.

_Answer here._
1. **Machine Learning Training:** Training complex machine learning models, especially deep learning models, often requires processing massive datasets and performing numerous computations. The related packages on the HPC CRAN task view may be the `h2o` package that connects to the h2o open source machine learning environment and has scalable implementations of random forests, GBM, GLM (with elastic net regularization), and deep learning.

2. **Monte Carlo Simulation**: Parallel computing techniques can accelerate Monte Carlo simulations by generating samples concurrently across multiple processors or nodes. The related packages on the HPC CRAN task view may be the `parallel` package that can be used to parallelize Monte Carlo simulations by distributing the computation of individual samples across multiple CPU cores, and the `foreach` package that can be used to parallelize Monte Carlo simulations by iterating over samples in parallel.

3. **Optimization Problems**: Solving optimization problems, such as linear programming or nonlinear optimization, often involves performing iterative computations to find the optimal solution. Parallel computing can accelerate the optimization process by concurrently evaluating multiple candidate solutions or performing parallel updates to optimization algorithms. The related packages on the HPC CRAN task view may be the `rgenoud` package by Mebane and Sekhon for genetic optimization using derivatives, and the `dclone` package that provides a global optimization approach.

## Problem 2: Pre-parallelization

The following functions can be written to be more efficient without using
`parallel`:

1. This function generates a `n x k` dataset with all its entries having a Poisson distribution with mean `lambda`.

```{r p2-fun1, eval = TRUE}
fun1 <- function(n = 100, k = 4, lambda = 4) {
  x <- NULL
  
  for (i in 1:n)
    x <- rbind(x, rpois(k, lambda))
  
  return(x)
}

fun1alt <- function(n = 100, k = 4, lambda = 4) {
  # YOUR CODE HERE
  matrix(rpois(n * k, lambda), ncol = k)
}

# Benchmarking
microbenchmark::microbenchmark(
  fun1(100),
  fun1alt(100)
)
```

How much faster?

_Answer here._
fun1(100) takes a mean of about 194.56632	microseconds, while fun1alt(100) only takes a mean of 23.49054 microseconds. This means that the vectorized version is about 171 microseconds faster thann the unvectorized version, which is about 7 times faster.


2.  Find the column max (hint: Checkout the function `max.col()`).

```{r p2-fun2, eval = TRUE}
# Data Generating Process (10 x 10,000 matrix)
set.seed(1234)
x <- matrix(rnorm(1e4), nrow=10)

# Find each column's max value
fun2 <- function(x) {
  apply(x, 2, max)
}

fun2alt <- function(x) {
  # YOUR CODE HERE
  x[cbind(max.col(t(x)), 1:ncol(x))]
}

# Benchmarking
bm <- microbenchmark::microbenchmark(
  fun2(x),
  fun2alt(x)
)
bm

# Plot
autoplot(bm)
```

_Answer here with a plot._
From the plot, we can see that fun2alt(x) is much faster than fun2(x) since its plot is on the left than the plot of fun2(x).


## Problem 3: Parallelize everything

We will now turn our attention to non-parametric 
[bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)).
Among its many uses, non-parametric bootstrapping allow us to obtain confidence
intervals for parameter estimates without relying on parametric assumptions.

The main assumption is that we can approximate many experiments by resampling
observations from our original dataset, which reflects the population. 

This function implements the non-parametric bootstrap:

```{r p3-boot-fun, eval = TRUE}
my_boot <- function(dat, stat, R, ncpus = 1L) {
  
  # Getting the random indices
  n <- nrow(dat)
  idx <- matrix(sample.int(n, n*R, TRUE), nrow=n, ncol=R)
 
  # Making the cluster using `ncpus`
  # STEP 1: GOES HERE
  cl <- makePSOCKcluster(ncpus)
  # STEP 2: GOES HERE
  clusterExport(cl, varlist = c("idx", "dat", "stat"), envir = environment())
  
  # STEP 3: THIS FUNCTION NEEDS TO BE REPLACED WITH parLapply
  ans <- parLapply(cl, seq_len(R), function(i) {
    stat(dat[idx[,i], , drop=FALSE])
  })
  
  # Coercing the list into a matrix
  ans <- do.call(rbind, ans)
  
  # STEP 4: GOES HERE
  stopCluster(cl)
  
  ans
  
}
```

1. Use the previous pseudocode, and make it work with `parallel`. Here is just an example
for you to try:

```{r p3-test-boot, eval = TRUE}
# Bootstrap of a linear regression model
my_stat <- function(d) {
  coef(lm(y ~ x, data = d)) 
}

# DATA SIM
set.seed(1)
n <- 500 
R <- 1e4
x <- cbind(rnorm(n)) 
y <- x*5 + rnorm(n)

# Check if we get something similar as lm
ans0 <- confint(lm(y~x))
ans1 <- my_boot(data.frame(x, y), my_stat, R, ncpus = 8)

ans0
t(apply(ans1, 2, quantile, probs = c(0.025, 0.975)))
```

2. Check whether your version actually goes faster than the non-parallel version:

```{r benchmark-problem3, eval = TRUE}
# your code here
parallel::detectCores()
system.time(my_boot(data.frame(x, y), my_stat, R, ncpus = 1))
system.time(my_boot(data.frame(x, y), my_stat, R, ncpus = 8))
```

_Answer here._
Using 8 CPUs instead of 1 CPU increases the user and system time, but decreases the elapsed time, which is the amount of time that has passed between the commencement and the completion of the function. Therefore, this version actually goes faster than the non-parallel version in runtime.

## Problem 4: Compile this markdown document using Rscript

Once you have saved this Rmd file, try running the following command
in your terminal:

```bash
Rscript --vanilla -e 'rmarkdown::render("[full-path-to-your-Rmd-file.Rmd]")' &
```

Where `[full-path-to-your-Rmd-file.Rmd]` should be replace with the full path to
your Rmd file... :).


